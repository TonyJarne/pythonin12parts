# Part 5: Scraping surgery

* The [ONS Web Scraping Policy](https://www.ons.gov.uk/aboutus/transparencyandgovernance/datastrategy/datapolicies/webscrapingpolicy) outlines how that organisation considers legal and ethical issues
* The [UK Statistics Authority Ethics Self-Asssessment Tool](https://uksa.statisticsauthority.gov.uk/the-authority-board/committees/national-statisticians-advisory-committees-and-panels/national-statisticians-data-ethics-advisory-committee/ethics-self-assessment-tool/) is referenced in that ONS guide and provides a useful framework
* It links to a [self-assessment form](https://uksa.statisticsauthority.gov.uk/wp-content/uploads/2021/04/2019_Self-Assessment_sheet_V2.1a.xlsx) and [guidance](https://uksa.statisticsauthority.gov.uk/wp-content/uploads/2021/04/2021_Self-assessment_guidance_V2.3.pdf), a copy of which can be found in this folder.
* You can normally find the Robots file by adding `/robots.txt` to the base URL of a website. For example the Bureau's Robots file is at https://www.thebureauinvestigates.com/robots.txt 
* Wikipedia's [page on the Robots exclusion standard](https://en.wikipedia.org/wiki/Robots_exclusion_standard) provides a useful guide to different aspects, such as the crawl-delay directive
* The Alternative Data Council working group [has a series of documents on "web data collection considerations"](https://fisd.net/alternative-data-council/)
* This video of a [webinar: Web Scraping From a Legal Perspective](https://www.bigmarker.com/oxylabs/web-scraping-legal-perspective?bmid=1b64733cb114) has some useful discussion points. My [notes here](https://github.com/paulbradshaw/MED7369-Specialist-Investigative-Journalism/blob/master/python/scrapinglaw.md)
